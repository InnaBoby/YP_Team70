## Распределение числовых данных
![distribution_of_numbers](/images/distribution_of_numbers.png)
К числовым переменным относятся:
- количество параграфов в контексте (num_of_ctx)
- количество слов в контексте (num_of_words)
- количество предложений в контексте (num_of_sents)
- длина ответа (в словах, answer_length)
- длина вопроса (в словах, question_length)

__Выводы:__
- Почти для каждого вопроса дано 10 параграфов контекста, чтобы найти ответ, наименьшее число данных контекстов - 2
- Меньше 10 параграфов имеют 838 вопросов
- В среднем слов в контексте ~1000, предложений ~50
- Длина ответа очень короткая, обычно немного больше 10, однако есть длинные ответы до 150 слов
- Длина вопроса в среднем около 25 слов

## Распределение текстовых переменных
![distribution_of_type_level](/images/distribution_of_level_type.png)

К текстовым переменным относятся:
- вопрос (question)
- ответ (answer)
- сложность вопроса (level)
- тип вопроса (type)

Есть повторяющиеся вопросы (7 вопросов). Среди них есть те, в которых есть полные повторения, и те, где отличается уровень вопроса. Так как уровень вопроса не требуется в обучении модели, можно оставить любой из повторяющихся вопросов.

__Выводы:__
- Больше всего вопросов имеют средний уровень сложности
- Тип вопроса "bridge" наиболее часто встречается в данных
- Среди ответов можно выделить наиболее частый, так как есть вопросы типа "yes/no", соответственно самые частые ответы также "yes/no"

## Анализ ответов (answer)
Среди ответов, действительно, самимы частыми являются "да" и "нет", однако так же есть короткие, одно-двусловные, ответы, являющиеся общими, например, названия географических объектов (United States, New York), профессий (film director), животных (dog).

В статье авторы также представляют распределения по типам ответов в процентах:
![answer_types](/images/answers_types.png)

__Наиболее частые наборы слов в ответах (н-граммы):__
- наиболее частое слово - the
- наиболее частая биграмма - of the
- наиболее частая триаграмм - new york city

__Наиболее частые наборы символов в ответах (н-граммы):__
- наиболее частый символ - пробел
- наиболее частая биграмма - an
- наиболее частая триаграмм - the

Касательно частоты встречаемости н-грамм слов и символов всё ожидаемо: артикли, как самое частое стоп-слово, а также набор одного из самых частых ответов - New York City

__Наиболее частая часть речи:__
Чаще всего в ответах встречаются именя собственные (PROPN) и существительные (NOUN). Также часто ответом являются числительные (NUM) - даты.

## Анализ вопросов
Понять о чем вопрос помогают вопросительные слова (_wh-слова_), а также вспомогательные слова (_is_, _are_, _was_...). На следующем облаке слов визуализация частых вопросительных и вспомогательных слов в вопросах.
![wordcloud_qw](/images/question_words_cloud.png)

Можно также визуализировать _триграммы вопроса_, чтобы определить основное направление того, о чем вопрос.
![wordcloud_tri](/images/question_trigrams_cloud.png)

__Наиболее частые наборы слов в ответах (н-граммы):__
- наиболее частое слово - the
- наиболее частая биграмма - of the
- наиболее частая триаграмм - what is the

__Наиболее частые наборы символов в ответах (н-граммы):__
- наиболее частый символ - пробел
- наиболее частая биграмма - e
- наиболее частая триаграмм -  th

__Наиболее частая часть речи:__
Среди частей речи в вопросах похожая ситуация, что и с ответами:
чаще всего встречаются именя собственные (PROPN) и существительные (NOUN). Также часто встречаются наречия, то есть вопросительные wh-слова и пунктуация, вопросы и запятые.

## Предобработка текста

Для решения задачи QA не требуется предобработка данных, так как для ответа на вопрос важно полностью понимать контекст, даже форма слова может нести значение (взять хотя бы _does_ и _did_, несущие в себе информацию о времени действия события). В стоп-слова входят вопросительные слова (wh-слова, о которых я писала выше), которые для задачи ответа на вопрос имеют чуть ли не ключевую роль.

Соотвественно для решения этой задачи предобработка текста, это подготовка к формату входа модели: соответствующая токенизация, деление на параграфы (если это BERT и подобные модели с сильно ограниченным входом) или наоборот склеивание (если это LLM с большим контекстом).
